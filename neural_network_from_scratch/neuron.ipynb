{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc22efaa",
   "metadata": {},
   "source": [
    "## Understanding Neurons, Weights, and Biases\n",
    "\n",
    "Before diving into code, let's intuitively understand how a **neuron** works in the context of artificial neural networks.\n",
    "\n",
    "### 🔬 What is a Neuron?\n",
    "\n",
    "A **neuron** is the fundamental building block of a neural network, inspired by the biological neurons in our brains. Just like a biological neuron receives signals, processes them, and sends out a response, an artificial neuron does something very similar — but mathematically.\n",
    "\n",
    "You can think of a neuron as a mini computational unit that:\n",
    "\n",
    "- Accepts multiple **inputs** (features of data),\n",
    "- Each input has an associated **weight** (importance),\n",
    "- It adds a **bias** (offset term), and\n",
    "- Then computes a final **output** by combining all these.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧮 Neuron Formula\n",
    "\n",
    "If we denote the inputs by:\n",
    "\n",
    "- $x_1, x_2, ..., x_n$\n",
    "\n",
    "and the corresponding weights by:\n",
    "\n",
    "- $w_1, w_2, ..., w_n$\n",
    "\n",
    "and introduce a bias term:\n",
    "\n",
    "- $b$\n",
    "\n",
    "Then the **output** of the neuron (before applying any activation function) is calculated as:\n",
    "\n",
    "$$\n",
    "z = w_1 \\cdot x_1 + w_2 \\cdot x_2 + \\dots + w_n \\cdot x_n + b\n",
    "$$\n",
    "\n",
    "This is essentially a **weighted sum** of the inputs plus the bias.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Intuition Behind Each Component\n",
    "\n",
    "- **Inputs ($x$)**: These are the features of your data. For example, if you're trying to predict the price of a house, your inputs might be the size, number of bedrooms, and location score.\n",
    "  \n",
    "- **Weights ($w$)**: These represent how important each input is. A higher weight means that input has more influence on the output. During training, the model learns the best weights that minimize prediction error.\n",
    "  \n",
    "- **Bias ($b$)**: The bias acts like a baseline or intercept. It helps the model make predictions even when all inputs are zero. In terms of geometry, it allows the model to shift the output up or down to better fit the data.\n",
    "\n",
    "- **Output ($z$)**: This is the result of the weighted sum of inputs and the bias. In more complex neural networks, this output is usually passed through an **activation function** (like sigmoid, ReLU, etc.) to introduce non-linearity, but we’ll focus on just the linear computation for now.\n",
    "\n",
    "---\n",
    "\n",
    "### ⚙️ Why This Matters\n",
    "\n",
    "Understanding how a single neuron works is crucial because **entire neural networks** are essentially made by stacking and connecting many such neurons together. Each neuron learns to detect or represent something meaningful in the data — and the network as a whole can learn highly complex patterns!\n",
    "\n",
    "---\n",
    "\n",
    "In the next section, we'll implement this neuron in Python and see how it computes outputs from given inputs, weights, and bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a1045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron(input: list, weights: list, bias: float = 0.0) -> float:\n",
    "    \"\"\"Simulate a neuron by calculating the weighted sum of inputs plus bias.\n",
    "    This function computes the dot product of the input and weights, adds a bias,\n",
    "    and returns the result.\n",
    "\n",
    "    Args:\n",
    "        input (list): A list of input values.\n",
    "        weights (list): A list of weights corresponding to the inputs.\n",
    "        bias (float, optional): A bias term associated with the neuron. Defaults to 0.0.\n",
    "\n",
    "    Returns:\n",
    "        output (float): The output of the neuron, which is the weighted sum of inputs plus bias.\n",
    "    \"\"\"\n",
    "    # Validate input and weights lengths\n",
    "    if len(input) != len(weights):\n",
    "        raise ValueError(\"Input and weights must have the same length.\")\n",
    "\n",
    "    # Calculate the weighted sum of inputs\n",
    "    output = sum(w * x for w, x in zip(weights, input)) + bias\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f9ccf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of the neuron: -2.55\n"
     ]
    }
   ],
   "source": [
    "# Checking the function with an example\n",
    "input_example = [0.5, 1.5, -2.0]\n",
    "weights_example = [0.2, -0.5, 1.0]\n",
    "bias_example = 0.1\n",
    "\n",
    "output_example = neuron(input_example, weights_example, bias_example)\n",
    "print(f\"Output of the neuron: {output_example}\")\n",
    "# Output of the neuron: 0.1 + (0.2 * 0.5) + (-0.5 * 1.5) + (1.0 * -2.0) = -2.55"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e939c8b",
   "metadata": {},
   "source": [
    "## 🧠 A Layer of Neurons\n",
    "\n",
    "Now that we understand how a **single neuron** works, we can extend the idea to a **layer of neurons**. In neural networks, a **layer** is simply a collection of neurons that process inputs in parallel. Each neuron in the layer has its **own set of weights and a bias**, but all neurons in the same layer receive the **same input vector**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧮 What Does a Layer Do?\n",
    "\n",
    "Imagine a scenario where we want to extract multiple features from the same input data. Instead of using a single neuron, we use **multiple neurons**, each tuned to detect different patterns. These neurons operate simultaneously, and the result is a vector of outputs — one from each neuron.\n",
    "\n",
    "If we have:\n",
    "- $m$ neurons in the layer,\n",
    "- $n$ inputs to each neuron ($x_1, x_2, \\dots, x_n$),\n",
    "\n",
    "then the **output of the $i^{\\text{th}}$ neuron** in the layer is:\n",
    "\n",
    "$$\n",
    "z_i = w_{i1} \\cdot x_1 + w_{i2} \\cdot x_2 + \\dots + w_{in} \\cdot x_n + b_i\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $w_{ij}$ is the weight associated with the $j^{\\text{th}}$ input of the $i^{\\text{th}}$ neuron,\n",
    "- $b_i$ is the bias term for the $i^{\\text{th}}$ neuron.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧾 Vector Representation of Layer Output\n",
    "\n",
    "To keep things compact and efficient, we can represent the outputs of all $m$ neurons as a **vector**:\n",
    "\n",
    "$$\n",
    "\\mathbf{z} = \\begin{bmatrix}\n",
    "z_1 \\\\\n",
    "z_2 \\\\\n",
    "\\vdots \\\\\n",
    "z_m\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This vector $\\mathbf{z}$ is the output of the entire layer and is typically passed on as input to the next layer in the network.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Intuition Behind a Layer of Neurons\n",
    "\n",
    "Each neuron in the layer focuses on capturing a different **feature** or **relationship** from the input data. For instance:\n",
    "- One neuron might focus on a linear combination that highlights a specific correlation.\n",
    "- Another might focus on a different aspect or subpattern.\n",
    "\n",
    "This **parallel processing** capability is powerful because it enables the model to learn **multiple perspectives** of the input data simultaneously. Think of it like a team of experts analyzing the same data but each using a different lens.\n",
    "\n",
    "---\n",
    "\n",
    "### ⚙️ Why Are Layers Important?\n",
    "\n",
    "Layers form the **core architecture of deep neural networks**. Here's why they matter:\n",
    "\n",
    "- A **single layer** can detect simple patterns.\n",
    "- **Multiple stacked layers** can detect increasingly complex and abstract patterns.\n",
    "- This **hierarchical learning** is what enables neural networks to perform exceptionally well on tasks like image recognition, language understanding, and time-series prediction.\n",
    "\n",
    "By stacking multiple layers, each learning different levels of abstraction, we get what we call a **deep network** — hence the term **deep learning**.\n",
    "\n",
    "---\n",
    "\n",
    "In the next section, we’ll see how to implement a layer of neurons in Python, building on the concept of a single neuron.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14b86b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from typing import List, Union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6319feed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_layer(\n",
    "    inputs: Union[float, List[float]],\n",
    "    weights: Union[float, List[float], List[List[float]]],\n",
    "    bias: Union[float, List[float]] = 0.0,\n",
    ") -> Union[float, List[float]]:\n",
    "    \"\"\"Simulate a layer of neurons by calculating the weighted sum of inputs plus bias for each neuron.\n",
    "    This function computes the dot product of inputs and weights for each neuron in the layer,\n",
    "\n",
    "    Args:\n",
    "        inputs (Union[float, List[float]]): Input values for the layer of neurons.\n",
    "        weights (Union[float, List[float], List[List[float]]]): Weights for the neurons in the layer.\n",
    "            If a single float is provided, it is assumed to be the weight for a single neuron.\n",
    "            If a list of floats is provided, it is assumed to be the weights for a single neuron.\n",
    "            If a list of lists is provided, each inner list represents the weights for a different neuron.\n",
    "        bias (Union[float, List[float]], optional): Bias term(s) for the neurons in the layer.\n",
    "            If a single float is provided, it is assumed to be the bias for all neurons.\n",
    "            If a list of floats is provided, each float represents the bias for a different neuron.\n",
    "            Defaults to 0.0.\n",
    "\n",
    "    Returns:\n",
    "        outputs (Union[float, List[float]]): The output of the layer of neurons, which is the weighted sum of inputs plus bias for each neuron.\n",
    "    \"\"\"\n",
    "    # Ensure inputs is a list\n",
    "    if isinstance(inputs, (int, float)):\n",
    "        inputs = [inputs]\n",
    "\n",
    "    # Ensure weights is a list of lists\n",
    "    if isinstance(weights, (int, float)):\n",
    "        weights = [[weights]]\n",
    "    if isinstance(weights, List) and not isinstance(weights[0], List):\n",
    "        weights = [weights]\n",
    "\n",
    "    # Ensure bias is a list\n",
    "    if isinstance(bias, (int, float)):\n",
    "        bias = [bias] * len(weights)\n",
    "    elif len(bias) != len(weights):\n",
    "        raise ValueError(\"Bias must match the number of neurons in the layer.\")\n",
    "\n",
    "    # Checking input and weights lengths\n",
    "    if len(inputs) != len(weights[0]):\n",
    "        raise ValueError(\"Input and weights must have the same length.\")\n",
    "\n",
    "    # Calculate the output for each neuron\n",
    "    outputs = []\n",
    "    for neuron_weights, neuron_bias in zip(weights, bias):\n",
    "        output = sum(w * x for w, x in zip(neuron_weights, inputs)) + neuron_bias\n",
    "        outputs.append(output)\n",
    "\n",
    "    return outputs if len(outputs) > 1 else outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9575dffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs of the neuron layer: [-2.55, 0.6000000000000001]\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "inputs_example = [0.5, 1.5, -2.0]\n",
    "weights_example_layer = [[0.2, -0.5, 1.0], [0.3, 0.1, -0.2]]\n",
    "bias_example_layer = [0.1, -0.1]\n",
    "\n",
    "outputs_example_layer = neuron_layer(\n",
    "    inputs_example, weights_example_layer, bias_example_layer\n",
    ")\n",
    "print(f\"Outputs of the neuron layer: {outputs_example_layer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
