{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d3feb06",
   "metadata": {},
   "source": [
    "# Building a Multi-Layer Perceptron (MLP) with NumPy\n",
    "\n",
    "In the previous [notebook](./neuron_using_numpy.ipynb), we implemented a single neuron from scratch using NumPy. We extended this to build a simple neural network with a single hidden layer that could process either a single input sample or a batch of samples.\n",
    "\n",
    "In this notebook, we take the next logical step by building a **Multi-Layer Perceptron (MLP)**—a feedforward neural network that consists of multiple layers, each with a customizable number of neurons. The MLP we construct here will be fully dynamic and can handle the following input configurations:\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Capabilities of the MLP Implementation\n",
    "\n",
    "The function you’ll build is designed to handle the following scenarios:\n",
    "\n",
    "1. **Single scalar input** (e.g., `3.2`)\n",
    "2. **Single input vector** (e.g., `[1.0, 2.0, 3.0]`)\n",
    "3. **Batch of input vectors** (e.g., `[[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]`)\n",
    "4. **Any number of hidden layers**, followed by the output layer\n",
    "5. **Variable number of neurons per layer**, with each layer having its own weight matrix and bias vector\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 Assumptions\n",
    "\n",
    "- The only external dependency is `NumPy`, which is a standard library for numerical computation in Python.\n",
    "- You can install it via:\n",
    "  ```bash\n",
    "  pip install numpy\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "## 📥 Expected User Inputs\n",
    "\n",
    "The user needs to provide the following:\n",
    "\n",
    "1. **Input (`x`)**: Can be:\n",
    "    - A float (scalar input)\n",
    "    - A list of floats (single input vector)\n",
    "    - A list of lists of floats (batch of input vectors)\n",
    "\n",
    "2. **`num_layers`**: An integer indicating the total number of layers (including both hidden and output layers).\n",
    "\n",
    "3. **Weights and Biases (per layer)**:\n",
    "    - Each layer’s **weights** and **biases** are to be provided as additional arguments.\n",
    "    - These are passed dynamically using the `*args` mechanism.\n",
    "    - The sequence should strictly alternate: `weights_layer1, bias_layer1, weights_layer2, bias_layer2, ..., weights_layerN, bias_layerN`.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Interpreting Weight and Bias Shapes\n",
    "\n",
    "The user should follow these conventions when providing weights and biases:\n",
    "\n",
    "### Weights:\n",
    "- If the input is a **scalar**, the weights for the first layer should be a list (e.g., `[0.5]` for 1 neuron).\n",
    "- If the input is a **vector of size `n`**, the weights for one neuron should be a list of length `n`, and for `m` neurons, a list of `m` such lists, i.e., a 2D list of shape `(m, n)`.\n",
    "- If the input is a **batch of vectors**, the weights should still follow the shape `(neurons, features)`, matching the input feature dimension.\n",
    "\n",
    "### Biases:\n",
    "- Each bias should be a list of floats of length equal to the number of neurons in that layer.\n",
    "- Scalar biases can be provided as a single float, in which case it will be broadcasted across all neurons in that layer.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Example Usage\n",
    "\n",
    "Let’s assume we want to build a 2-layer MLP (1 hidden layer with 3 neurons, and 1 output layer with 2 neurons):\n",
    "\n",
    "```python\n",
    "# Input: batch of 2 samples, each with 4 features\n",
    "input = [[0.1, 0.2, 0.3, 0.4],\n",
    "         [0.5, 0.6, 0.7, 0.8]]\n",
    "\n",
    "num_layers = 2\n",
    "\n",
    "# Layer 1: 3 neurons, each takes 4 input features\n",
    "weights1 = [[0.1, 0.2, 0.3, 0.4],\n",
    "            [0.5, 0.6, 0.7, 0.8],\n",
    "            [0.9, 1.0, 1.1, 1.2]]\n",
    "bias1 = [0.1, 0.2, 0.3]\n",
    "\n",
    "# Layer 2: 2 neurons, each takes 3 input features (from previous layer)\n",
    "weights2 = [[0.1, 0.2, 0.3],\n",
    "            [0.4, 0.5, 0.6]]\n",
    "bias2 = [0.1, 0.2]\n",
    "```\n",
    "\n",
    "These would be passed to the function as:\n",
    "\n",
    "```python\n",
    "output = forward_pass(input, num_layers, weights1, bias1, weights2, bias2)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Function Design: `mlp_using_numpy`\n",
    "\n",
    "The core function will use `*args` to accept dynamic weights and biases for all layers. It will:\n",
    "\n",
    "- Normalize the input into a NumPy array\n",
    "- Iterate through each layer\n",
    "- Apply a linear transformation (`z = x @ W.T + b`)\n",
    "- Use an activation function like ReLU (or identity for output) after each layer\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Error Handling Considerations\n",
    "\n",
    "The implementation should validate the following:\n",
    "\n",
    "- Total number of weight-bias pairs must match `num_layers * 2`.\n",
    "- Weight matrices should align with the dimensions of the input or previous layer’s output.\n",
    "- Bias vectors must match the number of neurons in their respective layers.\n",
    "- Input shapes are validated and reshaped where necessary for consistency.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "By the end of this notebook, you will have a flexible and modular MLP architecture that is built entirely from scratch using NumPy, with full support for variable input dimensions and arbitrary depth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9a5c7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "from typing import List, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ab1958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_using_numpy(\n",
    "    inputs: Union[float, List[float], List[List[float]]], num_layers: int, *args\n",
    ") -> Union[float, List[float], List[List[float]]]:\n",
    "    \"\"\"\n",
    "    Implements a simple Multi-Layer Perceptron (MLP) using NumPy, without activation functions.\n",
    "    Supports scalar, vector, or batch input and arbitrary number of layers with custom weights/biases.\n",
    "\n",
    "    The function expects arguments in the following sequence:\n",
    "    - inputs: scalar / list / list of lists\n",
    "    - num_layers: total number of layers (hidden + output)\n",
    "    - *args: A flat sequence alternating weights and biases for each layer:\n",
    "        (weights1, bias1, weights2, bias2, ..., weightsN, biasN)\n",
    "\n",
    "    Each weight should be a 2D list (neurons_in_layer, input_dim),\n",
    "    and each bias should be a 1D list (length = neurons_in_layer).\n",
    "\n",
    "    Args:\n",
    "        inputs (Union[float, List[float], List[List[float]]]):\n",
    "            Input to the network. Can be:\n",
    "                - float: scalar input\n",
    "                - list of floats: single sample\n",
    "                - list of list of floats: batch of samples\n",
    "        num_layers (int): Number of layers in the MLP (including output layer)\n",
    "        *args: Weights and biases for each layer, alternating as\n",
    "               weights1, bias1, weights2, bias2, ..., weightsN, biasN.\n",
    "\n",
    "    Returns:\n",
    "        Union[float, List[float], List[List[float]]]: Output of the MLP\n",
    "            - float if single scalar output\n",
    "            - list of floats for single sample\n",
    "            - list of list of floats for batch output\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If shape mismatches or incorrect number of arguments are detected.\n",
    "\n",
    "    Example:\n",
    "        >>> mlp_using_numpy([1.0, 2.0], 2,\n",
    "        ...                 [[0.5, 0.5], [0.5, 0.5]], [0.1, 0.1],\n",
    "        ...                 [[0.3, 0.3], [0.3, 0.3]], [0.2, 0.2])\n",
    "        [1.16, 1.16]\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert inputs to NumPy array and reshape if needed\n",
    "    inputs = np.array(inputs, dtype=float)\n",
    "\n",
    "    if inputs.ndim == 0:\n",
    "        inputs = inputs.reshape(1, 1)  # scalar input\n",
    "    elif inputs.ndim == 1:\n",
    "        inputs = inputs.reshape(1, -1)  # single input vector\n",
    "    elif inputs.ndim == 2:\n",
    "        pass  # batch input\n",
    "    else:\n",
    "        raise ValueError(\"Input must be a scalar, 1D list, or 2D list.\")\n",
    "\n",
    "    # Validate number of layers\n",
    "    if num_layers < 1:\n",
    "        raise ValueError(\"Number of layers must be at least 1.\")\n",
    "\n",
    "    # Validate number of weight/bias arguments\n",
    "    if len(args) != 2 * num_layers:\n",
    "        raise ValueError(\n",
    "            f\"Expected {2 * num_layers} arguments for {num_layers} layers (weights and biases), but got {len(args)}.\"\n",
    "        )\n",
    "\n",
    "    # Forward pass through each layer\n",
    "    output = inputs\n",
    "    for layer in range(num_layers):\n",
    "        weights = np.array(args[2 * layer], dtype=float)\n",
    "        biases = np.array(args[2 * layer + 1], dtype=float)\n",
    "\n",
    "        if weights.ndim != 2:\n",
    "            raise ValueError(f\"Layer {layer + 1} weights must be a 2D list or array.\")\n",
    "        if biases.ndim != 1:\n",
    "            raise ValueError(f\"Layer {layer + 1} biases must be a 1D list or array.\")\n",
    "\n",
    "        if weights.shape[1] != output.shape[1]:\n",
    "            raise ValueError(\n",
    "                f\"Shape mismatch at layer {layer + 1}: weight expects input of shape (*, {weights.shape[1]}), got (*, {output.shape[1]}).\"\n",
    "            )\n",
    "\n",
    "        if biases.shape[0] != weights.shape[0]:\n",
    "            raise ValueError(\n",
    "                f\"Shape mismatch: bias length ({biases.shape[0]}) does not match number of neurons ({weights.shape[0]}) at layer {layer + 1}.\"\n",
    "            )\n",
    "\n",
    "        # Forward step: output = input @ weights.T + bias\n",
    "        output = output @ weights.T + biases\n",
    "\n",
    "    # Output formatting\n",
    "    if output.shape[0] == 1:\n",
    "        if output.shape[1] == 1:\n",
    "            return float(output[0, 0])  # scalar output\n",
    "        return output.flatten().tolist()  # single sample output\n",
    "    else:\n",
    "        return output.tolist()  # batch output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7021dc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.16, 1.16]\n"
     ]
    }
   ],
   "source": [
    "output = mlp_using_numpy(\n",
    "    [1.0, 2.0],\n",
    "    2,\n",
    "    [[0.5, 0.5], [0.5, 0.5]],\n",
    "    [0.1, 0.1],\n",
    "    [[0.3, 0.3], [0.3, 0.3]],\n",
    "    [0.2, 0.2],\n",
    ")\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
